{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                    \n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / \"data\"\n",
    "\n",
    "train_path = data_dir / \"annotations/train.jsonl\"\n",
    "dev_path = data_dir / \"annotations/dev_unseen.jsonl\"\n",
    "test_path = data_dir / \"annotations/test_unseen.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  \n",
       "0   its their character not their color that matters  \n",
       "1  don't be afraid to love again everyone is not ...  \n",
       "2                           putting bows on your pet  \n",
       "3  i love everything and everybody! except for sq...  \n",
       "4  everybody loves chocolate chip cookies, even h...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Uses jsonl data to preprocess and serve \n",
    "    dictionary of multimodal tensors for model input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        img_dir,\n",
    "        image_transform,\n",
    "        text_transform,\n",
    "        balance=False,\n",
    "        dev_limit=None,\n",
    "        random_state=0,\n",
    "    ):\n",
    "\n",
    "        self.samples_frame = pd.read_json(\n",
    "            data_path, lines=True\n",
    "        )\n",
    "        self.dev_limit = dev_limit\n",
    "        if balance:\n",
    "            neg = self.samples_frame[\n",
    "                self.samples_frame.label.eq(0)\n",
    "            ]\n",
    "            pos = self.samples_frame[\n",
    "                self.samples_frame.label.eq(1)\n",
    "            ]\n",
    "            self.samples_frame = pd.concat(\n",
    "                [\n",
    "                    neg.sample(\n",
    "                        pos.shape[0], \n",
    "                        random_state=random_state\n",
    "                    ), \n",
    "                    pos\n",
    "                ]\n",
    "            )\n",
    "        if self.dev_limit:\n",
    "            if self.samples_frame.shape[0] > self.dev_limit:\n",
    "                self.samples_frame = self.samples_frame.sample(\n",
    "                    dev_limit, random_state=random_state\n",
    "                )\n",
    "        self.samples_frame = self.samples_frame.reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.samples_frame.img = self.samples_frame.apply(\n",
    "            lambda row: (img_dir / row.img), axis=1\n",
    "        )\n",
    "\n",
    "        # https://github.com/drivendataorg/pandas-path\n",
    "        if not self.samples_frame.img.path.exists().all():\n",
    "            raise FileNotFoundError\n",
    "        if not self.samples_frame.img.path.is_file().all():\n",
    "            raise TypeError\n",
    "            \n",
    "        self.image_transform = image_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This method is called when you do len(instance) \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        return len(self.samples_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This method is called when you do instance[key] \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
    "\n",
    "        image = Image.open(\n",
    "            self.samples_frame.loc[idx, \"img\"]\n",
    "        ).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        text = self.text_transform(img_id).squeeze()\n",
    "\n",
    "        if \"label\" in self.samples_frame.columns:\n",
    "            label = torch.Tensor(\n",
    "                [self.samples_frame.loc[idx, \"label\"]]\n",
    "            ).long().squeeze()\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text, \n",
    "                \"label\": label\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return T.Compose(\n",
    "        [\n",
    "            T.Resize(256),\n",
    "            T.RandomHorizontalFlip(0.1),\n",
    "            T.RandomRotation(10),\n",
    "            # T.ColorJitter(**self.hparams['augmentation']['color_jit']),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "def get_val_transforms():\n",
    "    return T.Compose(\n",
    "        [\n",
    "            T.Resize(256),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftext_features(_id):\n",
    "    with open(os.path.join('features/text/', str(_id)), 'rb') as f:\n",
    "        ftext = pickle.load(f)\n",
    "    ftext = ftext.detach()\n",
    "    ftext = ftext[:,:100,:]\n",
    "\n",
    "    ftext = torch.cat([\n",
    "        ftext, # (1, x, 1024)\n",
    "        torch.zeros(1, 100-ftext.size()[1], 1024)\n",
    "    ], dim=1)\n",
    "\n",
    "    ftext = ftext.reshape(ftext.size()[0], -1, 2048)\n",
    "\n",
    "    return ftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet101(pretrained=True)\n",
    "# modules=list(resnet.children())[:-1]\n",
    "# resnet=nn.Sequential(*modules)\n",
    "# for p in resnet.parameters():\n",
    "#     p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        loss_fn,\n",
    "        vision_module,\n",
    "        dropout_p,\n",
    "        \n",
    "    ):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.vision_module = vision_module\n",
    "        \n",
    "        self.text_fc1 = torch.nn.Linear(50, 1)\n",
    "        \n",
    "        self.combi_fc1 = torch.nn.Linear(2*2048, 1024)\n",
    "        self.combi_fc2 = torch.nn.Linear(1024, 1024)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(1024, num_classes)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, text, image, label=None):\n",
    "        \n",
    "        text = text.permute(0, 2, 1) # (N, 50, 2048) -> (N, 2048, 50)\n",
    "        text = torch.nn.functional.gelu(self.text_fc1(text))\n",
    "        \n",
    "        image_features = torch.nn.functional.gelu(\n",
    "            self.vision_module(image)\n",
    "        )\n",
    "        \n",
    "        combined = torch.cat(\n",
    "            [text.permute(0,2,1), image_features], dim=1\n",
    "        ) # (N, 2, 2048)\n",
    "        \n",
    "        combined = combined.reshape(-1, 2*2048)\n",
    "        combined = self.dropout(\n",
    "            torch.nn.functional.gelu(\n",
    "                self.combi_fc1(combined)\n",
    "            )\n",
    "        )\n",
    "        combined = torch.nn.functional.gelu(self.combi_fc2(combined))\n",
    "        \n",
    "        logits = self.fc(fused)\n",
    "        pred = torch.nn.functional.softmax(logits)\n",
    "        loss = (\n",
    "            self.loss_fn(pred, label) \n",
    "            if label is not None else label\n",
    "        )\n",
    "        return (pred, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# for the purposes of this post, we'll filter\n",
    "# much of the lovely logging info from our LightningModule\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "class HatefulMemesModel(pl.LightningModule):\n",
    "    def __init__(self, hparams, lr):\n",
    "        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
    "            # ok, there's one for-loop but it doesn't count\n",
    "            if data_key not in hparams.keys():\n",
    "                raise KeyError(\n",
    "                    f\"{data_key} is a required hparam in this model\"\n",
    "                )\n",
    "        \n",
    "        super(HatefulMemesModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        self.output_path = Path(\n",
    "            self.hparams.get(\"output_path\", \"model-outputs\")\n",
    "        )\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # instantiate transforms, datasets\n",
    "        self.text_transform = ftext_features,\n",
    "        self.train_dataset = self._build_dataset(\"train_path\")\n",
    "        self.dev_dataset = self._build_dataset(\"dev_path\")\n",
    "        \n",
    "        # set up model and training\n",
    "        self.model = self._build_model()\n",
    "        self.trainer_params = self._get_trainer_params()\n",
    "    \n",
    "    ## Required LightningModule Methods (when validating) ##\n",
    "    \n",
    "    def forward(self, text, image, label=None):\n",
    "        return self.model(text, image, label)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        preds, loss = self.forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        preds, loss = self.eval().forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"batch_val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack(\n",
    "            tuple(\n",
    "                output[\"batch_val_loss\"] \n",
    "                for output in outputs\n",
    "            )\n",
    "        ).mean()\n",
    "        \n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizers = [\n",
    "            torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=self.hparams.get(\"lr\", 0.001)\n",
    "            )\n",
    "        ]\n",
    "        schedulers = [\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizers[0]\n",
    "            )\n",
    "        ]\n",
    "        return optimizers, schedulers\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dev_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "    \n",
    "    ## Convenience Methods ##\n",
    "    \n",
    "    def fit(self):\n",
    "        self._set_seed(self.hparams.get(\"random_state\", 42))\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)\n",
    "        \n",
    "    def _set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _build_dataset(self, dataset_key):\n",
    "        return HatefulMemesDataset(\n",
    "            data_path=self.hparams.get(dataset_key, dataset_key),\n",
    "            img_dir=self.hparams.get(\"img_dir\"),\n",
    "            image_transform=get_train_transforms() if 'train' in str(dataset_key) else get_val_transforms(),\n",
    "            text_transform=self.text_transform,\n",
    "            # limit training samples only\n",
    "            dev_limit=(\n",
    "                self.hparams.get(\"dev_limit\", None) \n",
    "                if \"train\" in str(dataset_key) else None\n",
    "            ),\n",
    "            balance=False#True if \"train\" in str(dataset_key) else False,\n",
    "        )\n",
    "    \n",
    "    def _build_model(self):\n",
    "        vision_module = torchvision.models.resnet152(\n",
    "            pretrained=True\n",
    "        )\n",
    "        modules=list(vision_module.children())[:-1]\n",
    "        vision_module=nn.Sequential(*modules)\n",
    "        for p in vision_module.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        return LanguageAndVisionConcat(\n",
    "            num_classes=self.hparams.get(\"num_classes\", 2),\n",
    "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "            vision_module=vision_module,\n",
    "            dropout_p=self.hparams.get(\"dropout_p\", 0.2),\n",
    "        )\n",
    "    \n",
    "    def _get_trainer_params(self):\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            filepath=self.output_path,\n",
    "            monitor=self.hparams.get(\n",
    "                \"checkpoint_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            mode=self.hparams.get(\n",
    "                \"checkpoint_monitor_mode\", \"min\"\n",
    "            ),\n",
    "            verbose=self.hparams.get(\"verbose\", True)\n",
    "        )\n",
    "\n",
    "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "            monitor=self.hparams.get(\n",
    "                \"early_stop_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            min_delta=self.hparams.get(\n",
    "                \"early_stop_min_delta\", 0.001\n",
    "            ),\n",
    "            patience=self.hparams.get(\n",
    "                \"early_stop_patience\", 3\n",
    "            ),\n",
    "            verbose=self.hparams.get(\"verbose\", True),\n",
    "        )\n",
    "\n",
    "        trainer_params = {\n",
    "            \"checkpoint_callback\": checkpoint_callback,\n",
    "            \"early_stop_callback\": early_stop_callback,\n",
    "            \"default_save_path\": self.output_path,\n",
    "            \"accumulate_grad_batches\": self.hparams.get(\n",
    "                \"accumulate_grad_batches\", 1\n",
    "            ),\n",
    "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
    "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
    "            \"gradient_clip_val\": self.hparams.get(\n",
    "                \"gradient_clip_value\", 1\n",
    "            ),\n",
    "        }\n",
    "        return trainer_params\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def make_submission_frame(self, test_path):\n",
    "        test_dataset = self._build_dataset(test_path)\n",
    "        submission_frame = pd.DataFrame(\n",
    "            index=test_dataset.samples_frame.id,\n",
    "            columns=[\"proba\", \"label\"]\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16))\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            preds, _ = self.model.eval().to(\"cpu\")(\n",
    "                batch[\"text\"], batch[\"image\"]\n",
    "            )\n",
    "            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
    "            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
    "        submission_frame.proba = submission_frame.proba.astype(float)\n",
    "        submission_frame.label = submission_frame.label.astype(int)\n",
    "        return submission_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \n",
    "    # Required hparams\n",
    "    \"train_path\": train_path,\n",
    "    \"dev_path\": dev_path,\n",
    "    \"img_dir\": data_dir,\n",
    "    \n",
    "    # Optional hparams\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"dev_limit\": None,\n",
    "    \"lr\": 0.0005,\n",
    "    \"max_epochs\": 10,\n",
    "    \"n_gpu\": 0,\n",
    "    \"batch_size\": 4,\n",
    "    # allows us to \"simulate\" having larger batches \n",
    "    \"accumulate_grad_batches\": 16,\n",
    "    \"early_stop_patience\": 3,\n",
    "}\n",
    "\n",
    "hateful_memes_model = HatefulMemesModel(hparams=hparams)\n",
    "hateful_memes_model.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler.StepLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
